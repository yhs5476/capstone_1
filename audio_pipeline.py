#!/usr/bin/env python3
"""
ìŒì„± íŒŒì¼ ë…¸ì´ì¦ˆ ì œê±° â†’ Whisper STT â†’ í…ìŠ¤íŠ¸ ì €ì¥ íŒŒì´í”„ë¼ì¸
"""

import os
import sys
import glob
import logging
import subprocess
import shutil
from pathlib import Path
from datetime import datetime
import torch
import torchaudio
import whisper
import soundfile as sf
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('audio_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# SpeechBrain importë¥¼ ì¡°ê±´ë¶€ë¡œ ì²˜ë¦¬
try:
    from speechbrain.inference.enhancement import SpectralMaskEnhancement
    from speechbrain.utils.fetching import LocalStrategy
    SPEECHBRAIN_AVAILABLE = True
except ImportError as e:
    logger.warning(f"SpeechBrain ë¡œë”© ì‹¤íŒ¨: {e}")
    SPEECHBRAIN_AVAILABLE = False

class AudioPipeline:
    """ìŒì„± ë° ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, use_gpu=True, target_language=None):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            use_gpu (bool): GPU ì‚¬ìš© ì—¬ë¶€
            target_language (str): ëŒ€ìƒ ì–¸ì–´ ('ko', 'ja', 'en', None=ìë™ê°ì§€)
        """
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.device = "cuda" if self.use_gpu else "cpu"
        self.target_language = target_language
        
        # í´ë” ê²½ë¡œ ì„¤ì •
        self.audio_input_dir = Path("audio_input")
        self.audio_out_dir = Path("audio_out") 
        self.audio_output_dir = Path("audio_output")
        self.script_output_dir = Path("script_output")
        
        # í´ë” ìƒì„±
        self._create_directories()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.denoiser = None
        self.whisper_model = None
        
        # ì§€ì› ì–¸ì–´ ì •ë³´
        self.supported_languages = {
            'ko': 'í•œêµ­ì–´',
            'ja': 'æ—¥æœ¬èª',
            'en': 'English',
            'zh': 'ä¸­æ–‡',
            'es': 'EspaÃ±ol',
            'fr': 'FranÃ§ais',
            'de': 'Deutsch',
            'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹'
        }
        
        # ì§€ì› íŒŒì¼ í˜•ì‹
        self.audio_formats = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac']
        self.video_formats = ['.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv', '.wmv']
        self.supported_formats = self.audio_formats + self.video_formats
        
        # FFmpeg ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.ffmpeg_available = self._check_ffmpeg()
        
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
        if target_language:
            lang_name = self.supported_languages.get(target_language, target_language)
            logger.info(f"ëŒ€ìƒ ì–¸ì–´: {lang_name} ({target_language})")
        else:
            logger.info("ì–¸ì–´: ìë™ ê°ì§€ ëª¨ë“œ")
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for directory in [self.audio_input_dir, self.audio_out_dir, 
                         self.audio_output_dir, self.script_output_dir]:
            directory.mkdir(exist_ok=True)
            logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„±/í™•ì¸: {directory}")
    
    def _check_ffmpeg(self):
        """FFmpeg ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
        try:
            result = subprocess.run(['ffmpeg', '-version'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                logger.info("âœ… FFmpeg ì‚¬ìš© ê°€ëŠ¥")
                return True
            else:
                logger.warning("âš ï¸ FFmpeg ì‹¤í–‰ ì‹¤íŒ¨")
                return False
        except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
            logger.warning("âš ï¸ FFmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ê°€ ì œí•œë©ë‹ˆë‹¤.")
            return False
    
    def _is_video_file(self, file_path):
        """ë¹„ë””ì˜¤ íŒŒì¼ì¸ì§€ í™•ì¸"""
        return Path(file_path).suffix.lower() in self.video_formats
    
    def _is_audio_file(self, file_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ì¸ì§€ í™•ì¸"""
        return Path(file_path).suffix.lower() in self.audio_formats
    
    def extract_audio_from_video(self, video_file, output_audio_file=None):
        """
        FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        
        Args:
            video_file (str): ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_audio_file (str, optional): ì¶œë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: ì¶”ì¶œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        if not self.ffmpeg_available:
            raise RuntimeError("FFmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. FFmpegë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        video_path = Path(video_file)
        if not video_path.exists():
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_file}")
        
        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        if output_audio_file is None:
            output_audio_file = self.audio_input_dir / f"{video_path.stem}_extracted.wav"
        else:
            output_audio_file = Path(output_audio_file)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_audio_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹œì‘: {video_file}")
            
            # FFmpeg ëª…ë ¹ì–´ êµ¬ì„±
            cmd = [
                'ffmpeg',
                '-i', str(video_path),          # ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼
                '-vn',                          # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì œê±°
                '-acodec', 'pcm_s16le',         # 16-bit PCM ì¸ì½”ë”©
                '-ar', '16000',                 # 16kHz ìƒ˜í”Œë§ ë ˆì´íŠ¸
                '-ac', '1',                     # ëª¨ë…¸ ì±„ë„
                '-y',                           # ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°
                str(output_audio_file)          # ì¶œë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
            ]
            
            # FFmpeg ì‹¤í–‰
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                logger.info(f"âœ… ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ: {output_audio_file}")
                return str(output_audio_file)
            else:
                error_msg = f"FFmpeg ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {result.stderr}"
                logger.error(error_msg)
                raise RuntimeError(error_msg)
                
        except subprocess.TimeoutExpired:
            error_msg = "FFmpeg ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼ (5ë¶„)"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        except Exception as e:
            error_msg = f"ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            logger.error(error_msg)
            raise
    
    def _load_denoiser(self):
        """ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë“œ"""
        if self.denoiser is None:
            if not SPEECHBRAIN_AVAILABLE:
                logger.warning("SpeechBrainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©")
                self._load_denoiser_alternative()
                return
                
            try:
                logger.info("ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì¤‘...")
                
                # Windows ê¶Œí•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ LocalStrategy ì‚¬ìš©
                import os
                os.environ['SPEECHBRAIN_CACHE_STRATEGY'] = 'LOCAL'
                
                # ì ˆëŒ€ ê²½ë¡œë¡œ savedir ì„¤ì •
                savedir = os.path.abspath("pretrained_models/metricgan-plus-voicebank")
                
                self.denoiser = SpectralMaskEnhancement.from_hparams(
                    source="speechbrain/metricgan-plus-voicebank",
                    savedir=savedir,
                    run_opts={"device": self.device}
                )
                logger.info("ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.info("ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì¬ì‹œë„...")
                try:
                    self._load_denoiser_alternative()
                except Exception as e2:
                    logger.error(f"ëŒ€ì•ˆ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
                    raise e
    
    def _load_denoiser_alternative(self):
        """ëŒ€ì•ˆ ë…¸ì´ì¦ˆ ì œê±° ë°©ë²• (ê°„ë‹¨í•œ ìŠ¤í™íŠ¸ëŸ¼ í•„í„°ë§)"""
        logger.info("ëŒ€ì•ˆ ë…¸ì´ì¦ˆ ì œê±° ë°©ë²• ì‚¬ìš© (ê°„ë‹¨í•œ í•„í„°ë§)")
        self.denoiser = "simple_filter"  # í”Œë˜ê·¸ë¡œ ì‚¬ìš©
    
    def _simple_denoise(self, waveform, sample_rate):
        """ê°„ë‹¨í•œ ë…¸ì´ì¦ˆ ì œê±° (ìŠ¤í™íŠ¸ëŸ¼ í•„í„°ë§)"""
        try:
            # ê°„ë‹¨í•œ ê³ ì—­ í†µê³¼ í•„í„° ì ìš©
            from scipy.signal import butter, filtfilt
            
            # 80Hz ì´í•˜ ì €ì£¼íŒŒ ë…¸ì´ì¦ˆ ì œê±°
            nyquist = sample_rate / 2
            low_cutoff = 80 / nyquist
            b, a = butter(4, low_cutoff, btype='high')
            
            # í•„í„° ì ìš©
            filtered = filtfilt(b, a, waveform.numpy())
            
            # ì •ê·œí™”
            filtered = filtered / np.max(np.abs(filtered)) * 0.8
            
            return torch.from_numpy(filtered).float()
            
        except ImportError:
            logger.warning("scipyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°„ë‹¨í•œ ì •ê·œí™”ë§Œ ì ìš©")
            # ê°„ë‹¨í•œ ì •ê·œí™”ë§Œ ì ìš©
            normalized = waveform / torch.max(torch.abs(waveform)) * 0.8
            return normalized
    
    def _load_whisper(self, model_size="base"):
        """Whisper ëª¨ë¸ ë¡œë“œ"""
        if self.whisper_model is None:
            try:
                logger.info(f"Whisper ëª¨ë¸ ({model_size}) ë¡œë”© ì¤‘...")
                self.whisper_model = whisper.load_model(model_size, device=self.device)
                logger.info("Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Whisper ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
    
    def denoise_audio(self, input_file, output_file):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë…¸ì´ì¦ˆ ì œê±°
        
        Args:
            input_file (str): ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            output_file (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        try:
            logger.info(f"ë…¸ì´ì¦ˆ ì œê±° ì‹œì‘: {input_file}")
            
            # ëª¨ë¸ ë¡œë“œ
            self._load_denoiser()
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
            waveform, sample_rate = torchaudio.load(input_file)
            
            # ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¥¼ 16kHzë¡œ ë³€í™˜ (SpeechBrain ëª¨ë¸ ìš”êµ¬ì‚¬í•­)
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
                sample_rate = 16000
            
            # ë…¸ì´ì¦ˆ ì œê±° ìˆ˜í–‰
            if self.denoiser == "simple_filter":
                # ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©
                logger.info("ê°„ë‹¨í•œ í•„í„°ë§ ë°©ë²•ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°")
                enhanced_waveform = self._simple_denoise(waveform.squeeze(0), sample_rate)
                enhanced_waveform = enhanced_waveform.unsqueeze(0)
            else:
                # SpeechBrain ëª¨ë¸ ì‚¬ìš©
                waveform = waveform.to(self.device)
                enhanced_waveform = self.denoiser.enhance_batch(waveform.unsqueeze(0))
                enhanced_waveform = enhanced_waveform.squeeze(0).cpu()
            
            # ì¶œë ¥ íŒŒì¼ ì €ì¥
            torchaudio.save(output_file, enhanced_waveform, sample_rate)
            
            logger.info(f"ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            logger.error(f"ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨ ({input_file}): {e}")
            raise
    
    def transcribe_audio(self, audio_file, output_text_file, srt_file=None):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (STT)
        
        Args:
            audio_file (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_text_file (str): ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            srt_file (str, optional): SRT ìë§‰ íŒŒì¼ ê²½ë¡œ
        """
        try:
            logger.info(f"STT ì²˜ë¦¬ ì‹œì‘: {audio_file}")
            
            # Whisper ëª¨ë¸ ë¡œë“œ
            self._load_whisper()
            
            # ìŒì„± ì¸ì‹ ìˆ˜í–‰ (ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            transcribe_options = {
                "word_timestamps": True,
                "verbose": True
            }
            
            # ì–¸ì–´ ì„¤ì •
            if self.target_language:
                transcribe_options["language"] = self.target_language
                logger.info(f"ì§€ì •ëœ ì–¸ì–´ë¡œ STT ì²˜ë¦¬: {self.supported_languages.get(self.target_language, self.target_language)}")
            else:
                logger.info("ì–¸ì–´ ìë™ ê°ì§€ë¡œ STT ì²˜ë¦¬")
            
            result = self.whisper_model.transcribe(str(audio_file), **transcribe_options)
            
            # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            transcribed_text = result["text"].strip()
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
            self._save_transcript_with_timestamps(audio_file, output_text_file, result)
            
            # ê°„ë‹¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„+í™”ì ì •ë³´ íŒŒì¼ ìƒì„±
            simple_file = Path(output_text_file).parent / f"{Path(output_text_file).stem}_simple.txt"
            self._save_simple_transcript(simple_file, result)
            logger.info(f"ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„±: {simple_file}")
            
            # SRT ìë§‰ íŒŒì¼ ìƒì„± (ìš”ì²­ëœ ê²½ìš°)
            if srt_file:
                self._save_srt_file(srt_file, result)
                logger.info(f"SRT ìë§‰ íŒŒì¼ ìƒì„±: {srt_file}")
            
            logger.info(f"STT ì²˜ë¦¬ ì™„ë£Œ: {output_text_file}")
            logger.info(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {transcribed_text[:100]}...")
            
            return transcribed_text
            
        except Exception as e:
            logger.error(f"STT ì²˜ë¦¬ ì‹¤íŒ¨ ({audio_file}): {e}")
            raise
    
    def _save_transcript_with_timestamps(self, audio_file, output_text_file, result):
        """íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ì „ì‚¬ ê²°ê³¼ ì €ì¥"""
        with open(output_text_file, 'w', encoding='utf-8') as f:
            # í—¤ë” ì •ë³´
            f.write(f"íŒŒì¼ëª…: {Path(audio_file).name}\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì–¸ì–´: {result.get('language', 'unknown')}\n")
            f.write(f"ì´ ê¸¸ì´: {self._format_time(result.get('duration', 0))}\n")
            f.write("=" * 60 + "\n\n")
            
            # ì „ì²´ í…ìŠ¤íŠ¸
            f.write("ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸\n")
            f.write("-" * 30 + "\n")
            f.write(result["text"].strip() + "\n\n")
            
            # ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ìŠ¤íƒ¬í”„
            f.write("â° íƒ€ì„ìŠ¤íƒ¬í”„ë³„ í…ìŠ¤íŠ¸\n")
            f.write("-" * 30 + "\n")
            
            if "segments" in result:
                for i, segment in enumerate(result["segments"], 1):
                    start_time = self._format_time(segment["start"])
                    end_time = self._format_time(segment["end"])
                    text = segment["text"].strip()
                    
                    f.write(f"[{start_time} â†’ {end_time}] {text}\n")
                
                # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ (ê°€ëŠ¥í•œ ê²½ìš°)
                f.write("\nğŸ”¤ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„\n")
                f.write("-" * 30 + "\n")
                
                for segment in result["segments"]:
                    if "words" in segment:
                        for word_info in segment["words"]:
                            start_time = self._format_time(word_info["start"])
                            end_time = self._format_time(word_info["end"])
                            word = word_info["word"].strip()
                            confidence = word_info.get("probability", 0)
                            
                            f.write(f"[{start_time}-{end_time}] {word} (ì‹ ë¢°ë„: {confidence:.2f})\n")
            else:
                f.write("íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
    
    def _format_time(self, seconds):
        """ì´ˆë¥¼ MM:SS.mmm í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if seconds is None:
            return "00:00.000"
        
        minutes = int(seconds // 60)
        seconds = seconds % 60
        return f"{minutes:02d}:{seconds:06.3f}"
    
    def _save_srt_file(self, srt_file, result):
        """SRT ìë§‰ íŒŒì¼ ìƒì„± (í™”ì ì •ë³´ í¬í•¨)"""
        with open(srt_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                # ìŠ¤ë§ˆíŠ¸ í™”ì í• ë‹¹ ì‚¬ìš©
                speaker_assignments = self._assign_smart_speakers(result["segments"])
                
                subtitle_index = 1
                for i, segment in enumerate(result["segments"]):
                    start_time = self._format_srt_time(segment["start"])
                    end_time = self._format_srt_time(segment["end"])
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # í• ë‹¹ëœ í™”ì ì‚¬ìš©
                    speaker_name = speaker_assignments[i]
                    
                    f.write(f"{subtitle_index}\n")
                    f.write(f"{start_time} --> {end_time}\n")
                    f.write(f"{speaker_name}: {text}\n\n")
                    subtitle_index += 1
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ìë§‰ìœ¼ë¡œ
                duration = result.get("duration", 60)  # ê¸°ë³¸ 60ì´ˆ
                start_time = self._format_srt_time(0)
                end_time = self._format_srt_time(duration)
                text = result["text"].strip()
                
                f.write("1\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"í™”ìA: {text}\n\n")
    
    def _format_srt_time(self, seconds):
        """ì´ˆë¥¼ SRT í˜•ì‹ (HH:MM:SS,mmm)ìœ¼ë¡œ ë³€í™˜"""
        if seconds is None:
            return "00:00:00,000"
        
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        milliseconds = int((secs % 1) * 1000)
        secs = int(secs)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def _save_simple_transcript(self, simple_file, result):
        """ê°„ë‹¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„+í™”ì ì •ë³´ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        with open(simple_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                # ìŠ¤ë§ˆíŠ¸ í™”ì í• ë‹¹
                speaker_assignments = self._assign_smart_speakers(result["segments"])
                
                for i, segment in enumerate(result["segments"]):
                    start_time = self._format_time(segment["start"])
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # í• ë‹¹ëœ í™”ì ì‚¬ìš©
                    speaker_name = speaker_assignments[i]
                    
                    # [ì‹œê°„] í™”ì: í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    f.write(f"[{start_time}] {speaker_name}: {text}\n")
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
                f.write(f"[00:00.000] í™”ìA: {result['text'].strip()}\n")
    
    def _assign_smart_speakers(self, segments):
        """ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ í™”ì í• ë‹¹"""
        if not segments:
            return []
        
        speaker_assignments = []
        current_speaker = 'A'
        last_end_time = 0
        speaker_map = {}
        speaker_count = 0
        
        for i, segment in enumerate(segments):
            start_time = segment.get("start", 0)
            end_time = segment.get("end", start_time + 1)
            text = segment.get("text", "").strip()
            
            if not text:
                speaker_assignments.append(f"í™”ì{current_speaker}")
                continue
            
            # í™”ì ë³€ê²½ ì¡°ê±´ë“¤
            should_change_speaker = False
            curr_duration = end_time - start_time  # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ê³„ì‚°
            
            # 1. ê¸´ ì¹¨ë¬µ (2ì´ˆ ì´ìƒ) í›„ì—ëŠ” í™”ìê°€ ë°”ë€” ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            silence_duration = start_time - last_end_time
            if silence_duration > 2.0:
                should_change_speaker = True
            
            # 2. ë¬¸ì¥ ê¸¸ì´ê°€ ë§¤ìš° ë‹¤ë¥¸ ê²½ìš° (í™”ìì˜ ë§í•˜ê¸° íŒ¨í„´ ì°¨ì´)
            if i > 0:
                prev_duration = segments[i-1].get("end", 0) - segments[i-1].get("start", 0)
                duration_ratio = max(curr_duration, prev_duration) / (min(curr_duration, prev_duration) + 0.1)
                if duration_ratio > 2.5:  # ê¸¸ì´ ì°¨ì´ê°€ 2.5ë°° ì´ìƒ
                    should_change_speaker = True
            
            # 3. íŠ¹ì • íŒ¨í„´ ê°ì§€ (ì§ˆë¬¸, ëŒ€ë‹µ, ê°íƒ„ì‚¬ ë“±)
            text_lower = text.lower()
            question_patterns = ['?', 'ï¼Ÿ', 'ã©ã†', 'ãªã«', 'ãªã‚“', 'ä½•']
            response_patterns = ['ã¯ã„', 'ãã†', 'ã†ã‚“', 'ãˆãˆ', 'ã„ãˆ', 'ã„ã‚„']
            
            if i > 0:
                prev_text = segments[i-1].get("text", "").lower()
                # ì´ì „ì´ ì§ˆë¬¸ì´ê³  í˜„ì¬ê°€ ëŒ€ë‹µì¸ ê²½ìš°
                if any(p in prev_text for p in question_patterns) and any(p in text_lower for p in response_patterns):
                    should_change_speaker = True
            
            # 4. ì—°ì†ëœ ì§§ì€ ë°œí™” (ëŒ€í™”ì˜ íŠ¹ì„±)
            if curr_duration < 1.5 and silence_duration < 0.5 and i > 0:
                prev_duration = segments[i-1].get("end", 0) - segments[i-1].get("start", 0)
                if prev_duration < 1.5:  # ë‘˜ ë‹¤ ì§§ì€ ë°œí™”ë©´ í™”ìê°€ ë‹¤ë¥¼ ê°€ëŠ¥ì„±
                    should_change_speaker = True
            
            # í™”ì ë³€ê²½ ì‹¤í–‰
            if should_change_speaker and i > 0:
                # ê¸°ì¡´ í™”ìë“¤ ì¤‘ì—ì„œ ê°€ì¥ ì˜¤ë˜ ì•ˆ ë‚˜ì˜¨ í™”ì ì„ íƒ
                available_speakers = ['A', 'B', 'C']
                if current_speaker in available_speakers:
                    available_speakers.remove(current_speaker)
                
                if available_speakers:
                    # ê°€ì¥ ìµœê·¼ì— ì‚¬ìš©ë˜ì§€ ì•Šì€ í™”ì ì„ íƒ
                    for speaker in available_speakers:
                        speaker_name = f"í™”ì{speaker}"
                        if speaker_name not in [speaker_assignments[j] for j in range(max(0, i-3), i)]:
                            current_speaker = speaker
                            break
                    else:
                        current_speaker = available_speakers[0]
            
            speaker_name = f"í™”ì{current_speaker}"
            speaker_assignments.append(speaker_name)
            last_end_time = end_time
        
        return speaker_assignments
    
    def process_single_file(self, input_file):
        """
        ë‹¨ì¼ íŒŒì¼ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ (ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ ì§€ì›)
        
        Args:
            input_file (str): ì…ë ¥ ì˜¤ë””ì˜¤ ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        try:
            input_path = Path(input_file)
            file_stem = input_path.stem
            
            # íŒŒì¼ íƒ€ì… í™•ì¸ ë° ì „ì²˜ë¦¬
            if self._is_video_file(input_file):
                logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ íŒŒì¼ ê°ì§€: {input_file}")
                # ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
                audio_file = self.extract_audio_from_video(input_file)
                logger.info(f"ğŸ“„ ì²˜ë¦¬í•  ì˜¤ë””ì˜¤ íŒŒì¼: {audio_file}")
            elif self._is_audio_file(input_file):
                logger.info(f"ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì§€: {input_file}")
                audio_file = input_file
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {input_path.suffix}")
            
            # 1ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°
            denoised_file = self.audio_out_dir / f"{file_stem}_denoised.wav"
            self.denoise_audio(audio_file, str(denoised_file))
            
            # 2ë‹¨ê³„: audio_outputìœ¼ë¡œ ë³µì‚¬ (íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ìœ ì§€)
            output_audio_file = self.audio_output_dir / f"{file_stem}_denoised.wav"
            import shutil
            shutil.copy2(denoised_file, output_audio_file)
            
            # 3ë‹¨ê³„: STT ì²˜ë¦¬
            transcript_file = self.script_output_dir / f"{file_stem}_transcript.txt"
            srt_file = self.script_output_dir / f"{file_stem}_subtitle.srt"
            transcribed_text = self.transcribe_audio(output_audio_file, transcript_file, srt_file)
            
            logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {input_file}")
            return transcribed_text
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({input_file}): {e}")
            raise
    
    def process_all_files(self):
        """audio_input í´ë”ì˜ ëª¨ë“  ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì (ì˜¤ë””ì˜¤ + ë¹„ë””ì˜¤)
        file_extensions = ['*.wav', '*.mp3', '*.m4a', '*.flac', '*.ogg', '*.aac',
                          '*.mp4', '*.avi', '*.mov', '*.mkv', '*.webm', '*.flv', '*.wmv']
        
        media_files = []
        for ext in file_extensions:
            media_files.extend(glob.glob(str(self.audio_input_dir / ext)))
        
        if not media_files:
            logger.warning(f"audio_input í´ë”ì— ì²˜ë¦¬í•  ë¯¸ë””ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info(f"ì§€ì› í˜•ì‹: {', '.join(file_extensions)}")
            return
        
        # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
        audio_files = [f for f in media_files if self._is_audio_file(f)]
        video_files = [f for f in media_files if self._is_video_file(f)]
        
        logger.info(f"ì´ {len(media_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"  - ì˜¤ë””ì˜¤ íŒŒì¼: {len(audio_files)}ê°œ")
        logger.info(f"  - ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
        
        if video_files and not self.ffmpeg_available:
            logger.warning(f"âš ï¸ FFmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë¹„ë””ì˜¤ íŒŒì¼ {len(video_files)}ê°œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            media_files = audio_files
        
        results = []
        for i, media_file in enumerate(media_files, 1):
            try:
                file_type = "ğŸ¬ ë¹„ë””ì˜¤" if self._is_video_file(media_file) else "ğŸµ ì˜¤ë””ì˜¤"
                logger.info(f"[{i}/{len(media_files)}] {file_type} ì²˜ë¦¬ ì¤‘: {Path(media_file).name}")
                result = self.process_single_file(media_file)
                results.append({
                    'file': Path(media_file).name,
                    'type': 'video' if self._is_video_file(media_file) else 'audio',
                    'status': 'success',
                    'text': result
                })
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {Path(media_file).name} - {e}")
                results.append({
                    'file': Path(media_file).name,
                    'type': 'video' if self._is_video_file(media_file) else 'audio',
                    'status': 'failed',
                    'error': str(e)
                })
        
        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥
        self._save_processing_summary(results)
        
        logger.info("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
        return results
    
    def _save_processing_summary(self, results):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥"""
        summary_file = self.script_output_dir / f"processing_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=== ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ===\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ íŒŒì¼ ìˆ˜: {len(results)}\n")
            
            success_count = sum(1 for r in results if r['status'] == 'success')
            failed_count = len(results) - success_count
            
            f.write(f"ì„±ê³µ: {success_count}ê°œ\n")
            f.write(f"ì‹¤íŒ¨: {failed_count}ê°œ\n")
            f.write("-" * 50 + "\n\n")
            
            for result in results:
                f.write(f"íŒŒì¼: {result['file']}\n")
                f.write(f"ìƒíƒœ: {result['status']}\n")
                if result['status'] == 'success':
                    f.write(f"í…ìŠ¤íŠ¸: {result['text'][:200]}...\n")
                else:
                    f.write(f"ì˜¤ë¥˜: {result['error']}\n")
                f.write("-" * 30 + "\n")
        
        logger.info(f"ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥: {summary_file}")

def main(target_language=None):
    """
    ë©”ì¸ í•¨ìˆ˜
    
    Args:
        target_language (str): ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ ('ko', 'ja', 'en', etc.)
    """
    print("=== ìŒì„± ë° ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
    print("ğŸµ ì˜¤ë””ì˜¤: audio_input â†’ ë…¸ì´ì¦ˆì œê±° â†’ audio_out â†’ STT â†’ script_output")
    print("ğŸ¬ ë¹„ë””ì˜¤: video_input â†’ ì˜¤ë””ì˜¤ì¶”ì¶œ â†’ ë…¸ì´ì¦ˆì œê±° â†’ STT â†’ script_output")
    print()
    
    # ì–¸ì–´ ì„¤ì • ì•ˆë‚´
    if target_language:
        supported_languages = {
            'ko': 'í•œêµ­ì–´', 'ja': 'æ—¥æœ¬èª', 'en': 'English', 'zh': 'ä¸­æ–‡',
            'es': 'EspaÃ±ol', 'fr': 'FranÃ§ais', 'de': 'Deutsch', 'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹'
        }
        lang_name = supported_languages.get(target_language, target_language)
        print(f"ğŸŒ ëŒ€ìƒ ì–¸ì–´: {lang_name} ({target_language})")
    else:
        print("ğŸŒ ì–¸ì–´: ìë™ ê°ì§€ ëª¨ë“œ")
    print()
    
    try:
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        use_gpu = torch.cuda.is_available()
        if use_gpu:
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = AudioPipeline(use_gpu=use_gpu, target_language=target_language)
        
        # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
        results = pipeline.process_all_files()
        
        # ê²°ê³¼ ì¶œë ¥
        if results:
            success_count = sum(1 for r in results if r['status'] == 'success')
            print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(results)}ê°œ íŒŒì¼ ì„±ê³µ")
        else:
            print("\nâš ï¸  ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("audio_input í´ë”ì— ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

# ì‚¬ìš© ì˜ˆì‹œ:
# 
# 1. ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬:
#    audio_input/ í´ë”ì— WAV, MP3, M4A, FLAC, OGG íŒŒì¼ ì €ì¥ í›„ ì‹¤í–‰
#
# 2. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (FFmpeg í•„ìš”):
#    audio_input/ í´ë”ì— MP4, AVI, MOV, MKV, WEBM íŒŒì¼ ì €ì¥ í›„ ì‹¤í–‰
#    
# 3. íŠ¹ì • ì–¸ì–´ ì§€ì •:
#    pipeline = AudioPipeline(target_language='ko')  # í•œêµ­ì–´
#    pipeline = AudioPipeline(target_language='ja')  # ì¼ë³¸ì–´
#
# 4. ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬:
#    pipeline = AudioPipeline()
#    result = pipeline.process_single_file("video.mp4")
#
# 5. FFmpeg ì„¤ì¹˜ í™•ì¸:
#    pipeline = AudioPipeline()
#    print(f"FFmpeg ì‚¬ìš© ê°€ëŠ¥: {pipeline.ffmpeg_available}")
#
# ì¶œë ¥ íŒŒì¼:
# - audio_out/: ë…¸ì´ì¦ˆ ì œê±°ëœ ì˜¤ë””ì˜¤
# - script_output/: ì „ì‚¬ í…ìŠ¤íŠ¸ ë° SRT ìë§‰
# - processing_summary_*.txt: ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
