#!/usr/bin/env python3
"""
ìŒì„± íŒŒì¼ ë…¸ì´ì¦ˆ ì œê±° â†’ Whisper STT â†’ ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ â†’ í…ìŠ¤íŠ¸ ì €ì¥ íŒŒì´í”„ë¼ì¸

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:
- librosa: ê³ ê¸‰ ìŒì„± íŠ¹ì„± ì¶”ì¶œ (pip install librosa)
- scikit-learn: í´ëŸ¬ìŠ¤í„°ë§ (pip install scikit-learn)
- numpy: ê¸°ë³¸ ìˆ˜ì¹˜ ì—°ì‚° (pip install numpy)
"""

import os
import sys
import glob
import logging
import subprocess
import shutil
from pathlib import Path
from datetime import datetime
import torch
import torchaudio
import whisper
import soundfile as sf
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('audio_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# SpeechBrain importë¥¼ ì¡°ê±´ë¶€ë¡œ ì²˜ë¦¬
try:
    from speechbrain.inference.enhancement import SpectralMaskEnhancement
    from speechbrain.inference.speaker import EncoderClassifier
    from speechbrain.utils.fetching import LocalStrategy
    import numpy as np
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.metrics.pairwise import cosine_similarity
    SPEECHBRAIN_AVAILABLE = True
except ImportError as e:
    logger.warning(f"SpeechBrain ë¡œë”© ì‹¤íŒ¨: {e}")
    SPEECHBRAIN_AVAILABLE = False

class AudioPipeline:
    """ìŒì„± ë° ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, use_gpu=True, target_language=None):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            use_gpu (bool): GPU ì‚¬ìš© ì—¬ë¶€
            target_language (str): ëŒ€ìƒ ì–¸ì–´ ('ko', 'ja', 'en', None=ìë™ê°ì§€)
        """
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.device = "cuda" if self.use_gpu else "cpu"
        self.target_language = target_language
        
        # í´ë” ê²½ë¡œ ì„¤ì •
        self.audio_input_dir = Path("audio_input")
        self.audio_out_dir = Path("audio_out") 
        self.audio_output_dir = Path("audio_output")
        self.script_output_dir = Path("script_output")
        
        # í´ë” ìƒì„±
        self._create_directories()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.denoiser = None
        self.whisper_model = None
        self.speaker_encoder = None  # ECAPA-VOXCELEB í™”ìë¶„ë¦¬ ëª¨ë¸
        
        # ì§€ì› ì–¸ì–´ ì •ë³´
        self.supported_languages = {
            'ko': 'í•œêµ­ì–´',
            'ja': 'æ—¥æœ¬èª',
            'en': 'English',
            'zh': 'ä¸­æ–‡',
            'es': 'EspaÃ±ol',
            'fr': 'FranÃ§ais',
            'de': 'Deutsch',
            'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹'
        }
        
        # ì§€ì› íŒŒì¼ í˜•ì‹
        self.audio_formats = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac']
        self.video_formats = ['.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv', '.wmv']
        self.supported_formats = self.audio_formats + self.video_formats
        
        # FFmpeg ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.ffmpeg_available = self._check_ffmpeg()
        
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
        if target_language:
            lang_name = self.supported_languages.get(target_language, target_language)
            logger.info(f"ëŒ€ìƒ ì–¸ì–´: {lang_name} ({target_language})")
        else:
            logger.info("ì–¸ì–´: ìë™ ê°ì§€ ëª¨ë“œ")
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for directory in [self.audio_input_dir, self.audio_out_dir, 
                         self.audio_output_dir, self.script_output_dir]:
            directory.mkdir(exist_ok=True)
            logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„±/í™•ì¸: {directory}")
    
    def _check_ffmpeg(self):
        """FFmpeg ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
        try:
            result = subprocess.run(['ffmpeg', '-version'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                logger.info("âœ… FFmpeg ì‚¬ìš© ê°€ëŠ¥")
                return True
            else:
                logger.warning("âš ï¸ FFmpeg ì‹¤í–‰ ì‹¤íŒ¨")
                return False
        except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
            logger.warning("âš ï¸ FFmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ê°€ ì œí•œë©ë‹ˆë‹¤.")
            return False
    
    def _is_video_file(self, file_path):
        """ë¹„ë””ì˜¤ íŒŒì¼ì¸ì§€ í™•ì¸"""
        return Path(file_path).suffix.lower() in self.video_formats
    
    def _is_audio_file(self, file_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ì¸ì§€ í™•ì¸"""
        return Path(file_path).suffix.lower() in self.audio_formats
    
    def extract_audio_from_video(self, video_file, output_audio_file=None):
        """
        FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        
        Args:
            video_file (str): ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_audio_file (str, optional): ì¶œë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: ì¶”ì¶œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        if not self.ffmpeg_available:
            raise RuntimeError("FFmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. FFmpegë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        video_path = Path(video_file)
        if not video_path.exists():
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_file}")
        
        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        if output_audio_file is None:
            output_audio_file = self.audio_input_dir / f"{video_path.stem}_extracted.wav"
        else:
            output_audio_file = Path(output_audio_file)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_audio_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹œì‘: {video_file}")
            
            # FFmpeg ëª…ë ¹ì–´ êµ¬ì„±
            cmd = [
                'ffmpeg',
                '-i', str(video_path),          # ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼
                '-vn',                          # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì œê±°
                '-acodec', 'pcm_s16le',         # 16-bit PCM ì¸ì½”ë”©
                '-ar', '16000',                 # 16kHz ìƒ˜í”Œë§ ë ˆì´íŠ¸
                '-ac', '1',                     # ëª¨ë…¸ ì±„ë„
                '-y',                           # ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°
                str(output_audio_file)          # ì¶œë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
            ]
            
            # FFmpeg ì‹¤í–‰
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                logger.info(f"âœ… ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ: {output_audio_file}")
                return str(output_audio_file)
            else:
                error_msg = f"FFmpeg ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {result.stderr}"
                logger.error(error_msg)
                raise RuntimeError(error_msg)
                
        except subprocess.TimeoutExpired:
            error_msg = "FFmpeg ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼ (5ë¶„)"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        except Exception as e:
            error_msg = f"ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            logger.error(error_msg)
            raise
    
    def _load_denoiser(self):
        """ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë“œ"""
        if self.denoiser is None:
            if not SPEECHBRAIN_AVAILABLE:
                logger.warning("SpeechBrainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©")
                self._load_denoiser_alternative()
                return
                
            try:
                logger.info("ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì¤‘...")
                
                # Windows ê¶Œí•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ LocalStrategy ì‚¬ìš©
                import os
                os.environ['SPEECHBRAIN_CACHE_STRATEGY'] = 'LOCAL'
                
                # ì ˆëŒ€ ê²½ë¡œë¡œ savedir ì„¤ì •
                savedir = os.path.abspath("pretrained_models/metricgan-plus-voicebank")
                
                self.denoiser = SpectralMaskEnhancement.from_hparams(
                    source="speechbrain/metricgan-plus-voicebank",
                    savedir=savedir,
                    run_opts={"device": self.device}
                )
                logger.info("ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.info("ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì¬ì‹œë„...")
                try:
                    self._load_denoiser_alternative()
                except Exception as e2:
                    logger.error(f"ëŒ€ì•ˆ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
                    raise e
    
    def _load_denoiser_alternative(self):
        """ëŒ€ì•ˆ ë…¸ì´ì¦ˆ ì œê±° ë°©ë²• (ê°„ë‹¨í•œ ìŠ¤í™íŠ¸ëŸ¼ í•„í„°ë§)"""
        logger.info("ëŒ€ì•ˆ ë…¸ì´ì¦ˆ ì œê±° ë°©ë²• ì‚¬ìš© (ê°„ë‹¨í•œ í•„í„°ë§)")
        self.denoiser = "simple_filter"  # í”Œë˜ê·¸ë¡œ ì‚¬ìš©
    
    def _simple_denoise(self, waveform, sample_rate):
        """ê°„ë‹¨í•œ ë…¸ì´ì¦ˆ ì œê±° (ìŠ¤í™íŠ¸ëŸ¼ í•„í„°ë§)"""
        try:
            # ê°„ë‹¨í•œ ê³ ì—­ í†µê³¼ í•„í„° ì ìš©
            from scipy.signal import butter, filtfilt
            
            # 80Hz ì´í•˜ ì €ì£¼íŒŒ ë…¸ì´ì¦ˆ ì œê±°
            nyquist = sample_rate / 2
            low_cutoff = 80 / nyquist
            b, a = butter(4, low_cutoff, btype='high')
            
            # í•„í„° ì ìš©
            filtered = filtfilt(b, a, waveform.numpy())
            
            # ì •ê·œí™”
            filtered = filtered / np.max(np.abs(filtered)) * 0.8
            
            return torch.from_numpy(filtered).float()
            
        except ImportError:
            logger.warning("scipyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°„ë‹¨í•œ ì •ê·œí™”ë§Œ ì ìš©")
            # ê°„ë‹¨í•œ ì •ê·œí™”ë§Œ ì ìš©
            normalized = waveform / torch.max(torch.abs(waveform)) * 0.8
            return normalized
    
    def _load_whisper(self, model_size="large-v3"):
        """Whisper ëª¨ë¸ ë¡œë“œ"""
        if self.whisper_model is None:
            try:
                logger.info(f"Whisper ëª¨ë¸ ({model_size}) ë¡œë”© ì¤‘...")
                self.whisper_model = whisper.load_model(model_size, device=self.device)
                logger.info("Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Whisper ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
    
    def _load_speaker_encoder(self):
        """ECAPA-VOXCELEB í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ"""
        if self.speaker_encoder is None:
            if not SPEECHBRAIN_AVAILABLE:
                logger.warning("SpeechBrainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í™”ìë¶„ë¦¬ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
                
            try:
                logger.info("ECAPA-VOXCELEB í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì¤‘...")
                
                # ê¶Œí•œ ë¬¸ì œ ìš°íšŒë¥¼ ìœ„í•´ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                import tempfile
                temp_dir = tempfile.mkdtemp(prefix="speechbrain_")
                
                # ECAPA-VOXCELEB ëª¨ë¸ ë¡œë“œ
                self.speaker_encoder = EncoderClassifier.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    savedir=temp_dir,
                    run_opts={"device": self.device}
                )
                logger.info("ECAPA-VOXCELEB í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                return True
            except Exception as e:
                logger.error(f"í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.info("ê¶Œí•œ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜ ê·œì¹™ ê¸°ë°˜ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.speaker_encoder = None
                return False
        return True
    
    def denoise_audio(self, input_file, output_file):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë…¸ì´ì¦ˆ ì œê±°
        
        Args:
            input_file (str): ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            output_file (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        try:
            logger.info(f"ë…¸ì´ì¦ˆ ì œê±° ì‹œì‘: {input_file}")
            
            # ëª¨ë¸ ë¡œë“œ
            self._load_denoiser()
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
            waveform, sample_rate = torchaudio.load(input_file)
            
            # ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¥¼ 16kHzë¡œ ë³€í™˜ (SpeechBrain ëª¨ë¸ ìš”êµ¬ì‚¬í•­)
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
                sample_rate = 16000
            
            # ë…¸ì´ì¦ˆ ì œê±° ìˆ˜í–‰
            if self.denoiser == "simple_filter":
                # ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©
                logger.info("ê°„ë‹¨í•œ í•„í„°ë§ ë°©ë²•ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°")
                enhanced_waveform = self._simple_denoise(waveform.squeeze(0), sample_rate)
                enhanced_waveform = enhanced_waveform.unsqueeze(0)
            else:
                # SpeechBrain ëª¨ë¸ ì‚¬ìš©
                waveform = waveform.to(self.device)
                enhanced_waveform = self.denoiser.enhance_batch(waveform.unsqueeze(0))
                enhanced_waveform = enhanced_waveform.squeeze(0).cpu()
            
            # ì¶œë ¥ íŒŒì¼ ì €ì¥
            torchaudio.save(output_file, enhanced_waveform, sample_rate)
            
            logger.info(f"ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            logger.error(f"ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨ ({input_file}): {e}")
            raise
    
    def transcribe_audio(self, audio_file, output_text_file, srt_file=None):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (STT)
        
        Args:
            audio_file (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_text_file (str): ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            srt_file (str, optional): SRT ìë§‰ íŒŒì¼ ê²½ë¡œ
        """
        try:
            logger.info(f"STT ì²˜ë¦¬ ì‹œì‘: {audio_file}")
            
            # Whisper ëª¨ë¸ ë¡œë“œ
            self._load_whisper()
            
            # ìŒì„± ì¸ì‹ ìˆ˜í–‰ (ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            transcribe_options = {
                "word_timestamps": True,
                "verbose": True
            }
            
            # ì–¸ì–´ ì„¤ì •
            if self.target_language:
                transcribe_options["language"] = self.target_language
                logger.info(f"ì§€ì •ëœ ì–¸ì–´ë¡œ STT ì²˜ë¦¬: {self.supported_languages.get(self.target_language, self.target_language)}")
            else:
                logger.info("ì–¸ì–´ ìë™ ê°ì§€ë¡œ STT ì²˜ë¦¬")
            
            result = self.whisper_model.transcribe(str(audio_file), **transcribe_options)
            
            # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            transcribed_text = result["text"].strip()
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
            self._save_transcript_with_timestamps(audio_file, output_text_file, result)
            
            # ê°„ë‹¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„+í™”ì ì •ë³´ íŒŒì¼ ìƒì„±
            simple_file = Path(output_text_file).parent / f"{Path(output_text_file).stem}_simple.txt"
            self._save_simple_transcript(simple_file, result, audio_file)
            logger.info(f"ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„±: {simple_file}")
            
            # SRT ìë§‰ íŒŒì¼ ìƒì„± (ìš”ì²­ëœ ê²½ìš°)
            if srt_file:
                self._save_srt_file(srt_file, result, audio_file)
                logger.info(f"SRT ìë§‰ íŒŒì¼ ìƒì„±: {srt_file}")
            
            logger.info(f"STT ì²˜ë¦¬ ì™„ë£Œ: {output_text_file}")
            logger.info(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {transcribed_text[:100]}...")
            
            return transcribed_text
            
        except Exception as e:
            logger.error(f"STT ì²˜ë¦¬ ì‹¤íŒ¨ ({audio_file}): {e}")
            raise
    
    def _save_transcript_with_timestamps(self, audio_file, output_text_file, result):
        """íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ì „ì‚¬ ê²°ê³¼ ì €ì¥"""
        with open(output_text_file, 'w', encoding='utf-8') as f:
            # í—¤ë” ì •ë³´
            f.write(f"íŒŒì¼ëª…: {Path(audio_file).name}\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì–¸ì–´: {result.get('language', 'unknown')}\n")
            f.write(f"ì´ ê¸¸ì´: {self._format_time(result.get('duration', 0))}\n")
            f.write("=" * 60 + "\n\n")
            
            # ì „ì²´ í…ìŠ¤íŠ¸
            f.write("ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸\n")
            f.write("-" * 30 + "\n")
            f.write(result["text"].strip() + "\n\n")
            
            # ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ìŠ¤íƒ¬í”„
            f.write("â° íƒ€ì„ìŠ¤íƒ¬í”„ë³„ í…ìŠ¤íŠ¸\n")
            f.write("-" * 30 + "\n")
            
            if "segments" in result:
                for i, segment in enumerate(result["segments"], 1):
                    start_time = self._format_time(segment["start"])
                    end_time = self._format_time(segment["end"])
                    text = segment["text"].strip()
                    
                    f.write(f"[{start_time} â†’ {end_time}] {text}\n")
                
                # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ (ê°€ëŠ¥í•œ ê²½ìš°)
                f.write("\nğŸ”¤ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„\n")
                f.write("-" * 30 + "\n")
                
                for segment in result["segments"]:
                    if "words" in segment:
                        for word_info in segment["words"]:
                            start_time = self._format_time(word_info["start"])
                            end_time = self._format_time(word_info["end"])
                            word = word_info["word"].strip()
                            confidence = word_info.get("probability", 0)
                            
                            f.write(f"[{start_time}-{end_time}] {word} (ì‹ ë¢°ë„: {confidence:.2f})\n")
            else:
                f.write("íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
    
    def _format_time(self, seconds):
        """ì´ˆë¥¼ MM:SS.mmm í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if seconds is None:
            return "00:00.000"
        
        minutes = int(seconds // 60)
        seconds = seconds % 60
        return f"{minutes:02d}:{seconds:06.3f}"
    
    def _save_srt_file(self, srt_file, result, audio_file=None):
        """SRT ìë§‰ íŒŒì¼ ìƒì„± (í™”ì ì •ë³´ í¬í•¨)"""
        with open(srt_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                # ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‚¬ìš©
                if audio_file:
                    # í˜„ì¬ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´ë¥¼ ì„ì‹œ ì €ì¥
                    self._current_audio_file = audio_file
                    speaker_assignments = self._assign_smart_speakers(result["segments"])
                    # ì„ì‹œ ì •ë³´ ì œê±°
                    delattr(self, '_current_audio_file')
                else:
                    speaker_assignments = self._assign_smart_speakers(result["segments"])
                
                subtitle_index = 1
                for i, segment in enumerate(result["segments"]):
                    start_time = self._format_srt_time(segment["start"])
                    end_time = self._format_srt_time(segment["end"])
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # í• ë‹¹ëœ í™”ì ì‚¬ìš©
                    speaker_name = speaker_assignments[i]
                    
                    f.write(f"{subtitle_index}\n")
                    f.write(f"{start_time} --> {end_time}\n")
                    f.write(f"{speaker_name}: {text}\n\n")
                    subtitle_index += 1
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ìë§‰ìœ¼ë¡œ
                duration = result.get("duration", 60)  # ê¸°ë³¸ 60ì´ˆ
                start_time = self._format_srt_time(0)
                end_time = self._format_srt_time(duration)
                text = result["text"].strip()
                
                f.write("1\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"í™”ìA: {text}\n\n")
    
    def _format_srt_time(self, seconds):
        """ì´ˆë¥¼ SRT í˜•ì‹ (HH:MM:SS,mmm)ìœ¼ë¡œ ë³€í™˜"""
        if seconds is None:
            return "00:00:00,000"
        
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        milliseconds = int((secs % 1) * 1000)
        secs = int(secs)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def _save_simple_transcript(self, simple_file, result, audio_file=None):
        """ê°„ë‹¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„+í™”ì ì •ë³´ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        with open(simple_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                # ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‚¬ìš©
                if audio_file:
                    logger.info("ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‹œì‘...")
                    # í˜„ì¬ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´ë¥¼ ì„ì‹œ ì €ì¥
                    self._current_audio_file = audio_file
                    speaker_assignments = self._assign_smart_speakers(result["segments"])
                    # ì„ì‹œ ì •ë³´ ì œê±°
                    delattr(self, '_current_audio_file')
                else:
                    logger.info("ëŒ€ì•ˆ í™”ì í• ë‹¹ ì‚¬ìš©...")
                    speaker_assignments = self._assign_smart_speakers(result["segments"])
                
                for i, segment in enumerate(result["segments"]):
                    start_time = self._format_time(segment["start"])
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # í• ë‹¹ëœ í™”ì ì‚¬ìš©
                    speaker_name = speaker_assignments[i]
                    
                    # [ì‹œê°„] í™”ì: í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    f.write(f"[{start_time}] {speaker_name}: {text}\n")
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
                f.write(f"[00:00.000] í™”ìA: {result['text'].strip()}\n")
    
    def _assign_smart_speakers(self, segments):
        """ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ì ë¶„ë¦¬"""
        if not segments:
            return []
        
        logger.info(f"ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì‹œì‘ - ì´ {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        
        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ë‹¨ì¼ í™”ì
        if len(segments) <= 1:
            logger.info("ì„¸ê·¸ë¨¼íŠ¸ 1ê°œ ì´í•˜ - ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬")
            return ["í™”ìA" for _ in segments]
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì‹œë„
        audio_file = getattr(self, '_current_audio_file', None)
        if audio_file:
            return self._voice_feature_based_assignment(audio_file, segments)
        else:
            # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¡œì§
            return self._fallback_speaker_assignment(segments)
    
    def _voice_feature_based_assignment(self, audio_file, segments):
        """ìŒì„± íŠ¹ì„±(ì£¼íŒŒìˆ˜, í”¼ì¹˜, ìŠ¤í™íŠ¸ëŸ¼) ê¸°ë°˜ í™”ì ë¶„ë¦¬ + ë…ë°± ì²˜ë¦¬"""
        try:
            logger.info("ìŒì„± íŠ¹ì„± ì¶”ì¶œ ë° ë…ë°± ë¶„ì„ ì¤‘...")
            
            # ë…ë°± ì—¬ë¶€ ì‚¬ì „ íŒë‹¨
            is_monologue = self._detect_monologue_pattern(segments)
            if is_monologue:
                logger.info("ë…ë°± íŒ¨í„´ ê°ì§€ - ë…ë°± ì „ìš© ì²˜ë¦¬ ëª¨ë“œ")
                return self._handle_monologue_segments(segments)
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
            waveform, sample_rate = torchaudio.load(audio_file)
            
            # ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # ì„¸ê·¸ë¨¼íŠ¸ë³„ ìŒì„± íŠ¹ì„± ì¶”ì¶œ
            voice_features = []
            valid_segments = []
            
            for i, segment in enumerate(segments):
                start_time = segment.get("start", 0)
                end_time = segment.get("end", start_time + 1)
                text = segment.get("text", "").strip()
                
                if not text or end_time <= start_time:
                    continue
                
                # ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ ì¶”ì¶œ
                start_sample = int(start_time * sample_rate)
                end_sample = int(end_time * sample_rate)
                
                if start_sample >= waveform.shape[1] or end_sample <= start_sample:
                    continue
                
                segment_audio = waveform[:, start_sample:end_sample]
                
                # ë„ˆë¬´ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê±´ë„ˆë›°ê¸° (ìµœì†Œ 0.3ì´ˆ)
                if segment_audio.shape[1] < sample_rate * 0.3:
                    continue
                
                # ìŒì„± íŠ¹ì„± ì¶”ì¶œ
                features = self._extract_voice_features(segment_audio.squeeze(), sample_rate)
                if features is not None:
                    voice_features.append(features)
                    valid_segments.append((i, segment))
            
            if len(voice_features) < 2:
                logger.info("ìœ íš¨í•œ ìŒì„± íŠ¹ì„±ì´ ë¶€ì¡± - ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬")
                return ["í™”ìA" for _ in segments]
            
            # ìŒì„± íŠ¹ì„± ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
            speaker_labels = self._cluster_voice_features(voice_features)
            
            # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì í• ë‹¹
            return self._assign_speakers_from_clusters(segments, valid_segments, speaker_labels)
            
        except Exception as e:
            logger.error(f"ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            return self._fallback_speaker_assignment(segments)
    
    def _detect_monologue_pattern(self, segments):
        """ë…ë°± íŒ¨í„´ ê°ì§€ (ê°œì„ ëœ ë²„ì „)"""
        try:
            logger.info(f"ë…ë°± íŒ¨í„´ ë¶„ì„ ì¤‘... (ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(segments)})")
            
            # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì ì–´ë„ ë…ë°± ê°€ëŠ¥ì„± ê²€í† 
            if len(segments) < 2:
                logger.info("ì„¸ê·¸ë¨¼íŠ¸ 1ê°œ - ë…ë°±ìœ¼ë¡œ íŒë‹¨")
                return True
            
            # 1. ì¹¨ë¬µ ì‹œê°„ ë¶„ì„
            silence_durations = []
            long_silences = 0  # 2ì´ˆ ì´ìƒ ì¹¨ë¬µ (ê¸°ì¤€ ì™„í™”)
            very_long_silences = 0  # 5ì´ˆ ì´ìƒ ì¹¨ë¬µ
            
            for i in range(1, len(segments)):
                prev_end = segments[i-1].get("end", 0)
                curr_start = segments[i].get("start", 0)
                silence = curr_start - prev_end
                silence_durations.append(silence)
                
                if silence > 5.0:
                    very_long_silences += 1
                elif silence > 2.0:
                    long_silences += 1
            
            avg_silence = sum(silence_durations) / len(silence_durations) if silence_durations else 0
            
            # 2. ë°œí™” ê¸¸ì´ ë¶„ì„
            segment_durations = []
            for segment in segments:
                duration = segment.get("end", 0) - segment.get("start", 0)
                segment_durations.append(duration)
            
            avg_duration = sum(segment_durations) / len(segment_durations)
            max_duration = max(segment_durations) if segment_durations else 0
            
            # 3. í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„
            total_text_length = sum(len(segment.get("text", "")) for segment in segments)
            avg_text_length = total_text_length / len(segments)
            
            # 4. í™”ì ë³€ê²½ ì‹ í˜¸ ë¶„ì„
            speaker_change_signals = 0
            for i in range(1, len(segments)):
                curr_text = segments[i].get("text", "").strip()
                prev_text = segments[i-1].get("text", "").strip()
                
                # ëŒ€í™” ì‹ í˜¸ í‚¤ì›Œë“œ (ì§ˆë¬¸-ì‘ë‹µ íŒ¨í„´)
                question_words = ['?', 'ï¼Ÿ', 'ë­', 'ë¬´ì—‡', 'ãªã«', 'what', 'how']
                response_words = ['ë„¤', 'ì˜ˆ', 'ì•„ë‹ˆ', 'ã¯ã„', 'ãã†', 'yes', 'no']
                
                if (any(q in prev_text for q in question_words) and 
                    any(r in curr_text for r in response_words)):
                    speaker_change_signals += 1
            
            # 5. ë…ë°± íŒë‹¨ ê¸°ì¤€ (ë” ê´€ëŒ€í•˜ê²Œ)
            monologue_indicators = [
                very_long_silences == 0,               # ë§¤ìš° ê¸´ ì¹¨ë¬µ(5ì´ˆ+)ì´ ì—†ìŒ
                avg_silence < 2.0,                     # í‰ê·  ì¹¨ë¬µì´ 2ì´ˆ ë¯¸ë§Œ (ì™„í™”)
                avg_duration > 1.5 or max_duration > 4.0,  # í‰ê·  1.5ì´ˆ+ ë˜ëŠ” ìµœëŒ€ 4ì´ˆ+
                avg_text_length > 10,                  # í‰ê·  í…ìŠ¤íŠ¸ê°€ 10ì ì´ìƒ (ì™„í™”)
                speaker_change_signals == 0,           # í™”ì ë³€ê²½ ì‹ í˜¸ê°€ ì—†ìŒ
                len(segments) <= 5                     # ì„¸ê·¸ë¨¼íŠ¸ê°€ 5ê°œ ì´í•˜ (ë…ë°±ì€ ë³´í†µ ì ìŒ)
            ]
            
            monologue_score = sum(monologue_indicators)
            
            logger.info(f"ë…ë°± ë¶„ì„ ê²°ê³¼: ì ìˆ˜ {monologue_score}/6 "
                       f"(ë§¤ìš°ê¸´ì¹¨ë¬µ: {very_long_silences}, ê¸´ì¹¨ë¬µ: {long_silences}, "
                       f"í‰ê· ì¹¨ë¬µ: {avg_silence:.1f}ì´ˆ, í‰ê· ë°œí™”: {avg_duration:.1f}ì´ˆ, "
                       f"ìµœëŒ€ë°œí™”: {max_duration:.1f}ì´ˆ, í‰ê· í…ìŠ¤íŠ¸: {avg_text_length:.1f}ì, "
                       f"í™”ìë³€ê²½ì‹ í˜¸: {speaker_change_signals})")
            
            # 6ê°œ ì¤‘ 4ê°œ ì´ìƒ ë§Œì¡±í•˜ë©´ ë…ë°±ìœ¼ë¡œ íŒë‹¨
            is_monologue = monologue_score >= 4
            
            if is_monologue:
                logger.info("ğŸ¤ ë…ë°±ìœ¼ë¡œ íŒë‹¨ë¨ - ë…ë°± ì „ìš© ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™”")
            else:
                logger.info("ğŸ’¬ ëŒ€í™”ë¡œ íŒë‹¨ë¨ - ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì§„í–‰")
            
            return is_monologue
            
        except Exception as e:
            logger.error(f"ë…ë°± íŒ¨í„´ ê°ì§€ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ë…ë°±ìœ¼ë¡œ ì²˜ë¦¬ (ë‹¨ì¼ í™”ì ìš°ì„ )
            logger.info("ì˜¤ë¥˜ë¡œ ì¸í•´ ë…ë°±ìœ¼ë¡œ ì²˜ë¦¬")
            return True
    
    def _handle_monologue_segments(self, segments):
        """ë…ë°± ì„¸ê·¸ë¨¼íŠ¸ ì „ìš© ì²˜ë¦¬ - ë‹¨ì¼ í™”ì ìœ ì§€"""
        try:
            logger.info("ğŸ¤ ë…ë°± ëª¨ë“œ: ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬")
            
            # ë…ë°±ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë‹¨ì¼ í™”ì (í™”ìA)
            speaker_assignments = ["í™”ìA" for _ in segments]
            
            # ë…ë°± ë‚´ì—ì„œë„ ëª…í™•í•œ ì£¼ì œ ì „í™˜ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í™”ì ë¶„ë¦¬
            topic_changes = self._detect_strong_topic_changes(segments)
            
            if topic_changes:
                logger.info(f"ë…ë°± ë‚´ ê°•í•œ ì£¼ì œ ì „í™˜ ê°ì§€: {len(topic_changes)}ê°œ ì§€ì ")
                current_speaker = 'A'
                
                for i, segment in enumerate(segments):
                    if i in topic_changes:
                        current_speaker = 'B' if current_speaker == 'A' else 'A'
                        logger.info(f"ì„¸ê·¸ë¨¼íŠ¸ {i}: ê°•í•œ ì£¼ì œ ì „í™˜ â†’ í™”ì{current_speaker}")
                    
                    speaker_assignments[i] = f"í™”ì{current_speaker}"
            else:
                logger.info("ì£¼ì œ ì „í™˜ ì—†ìŒ - ì™„ì „í•œ ë‹¨ì¼ í™”ì ìœ ì§€")
            
            # ë…ë°± ê²°ê³¼ ë¡œê¹…
            from collections import Counter
            speaker_count = Counter(speaker_assignments)
            logger.info(f"ğŸ¤ ë…ë°± ì²˜ë¦¬ ì™„ë£Œ: {dict(speaker_count)}")
            
            return speaker_assignments
            
        except Exception as e:
            logger.error(f"ë…ë°± ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ["í™”ìA" for _ in segments]
    
    def _detect_strong_topic_changes(self, segments):
        """ë…ë°± ë‚´ ê°•í•œ ì£¼ì œ ì „í™˜ë§Œ ê°ì§€ (ë§¤ìš° ì—„ê²©í•œ ê¸°ì¤€)"""
        try:
            topic_changes = []
            
            # ë§¤ìš° ê°•í•œ ì£¼ì œ ì „í™˜ ì‹ í˜¸ë§Œ ê°ì§€
            strong_transition_keywords = [
                # í•œêµ­ì–´ - ëª…í™•í•œ ì „í™˜
                'ê·¸ëŸ°ë° ë§ì´ì•¼', 'ì•„ ê·¸ë¦¬ê³ ', 'ì°¸ ê·¸ëŸ°ë°', 'ì•„ ë§ë‹¤', 'ê·¸ê±´ ê·¸ë ‡ê³ ',
                # ì¼ë³¸ì–´ - ëª…í™•í•œ ì „í™˜  
                'ã¨ã“ã‚ã§', 'ãã†ã„ãˆã°', 'ã‚ã€ãã†ãã†', 'ãã‚Œã¯ãã†ã¨',
                # ì˜ì–´ - ëª…í™•í•œ ì „í™˜
                'by the way', 'speaking of', 'oh and', 'that reminds me'
            ]
            
            for i in range(1, len(segments)):
                curr_text = segments[i].get("text", "").strip().lower()
                
                # ê°•í•œ ì „í™˜ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°ë§Œ
                if any(keyword in curr_text for keyword in strong_transition_keywords):
                    topic_changes.append(i)
                    logger.debug(f"ê°•í•œ ì£¼ì œ ì „í™˜ ê°ì§€: ì„¸ê·¸ë¨¼íŠ¸ {i}")
            
            logger.info(f"ê°•í•œ ì£¼ì œ ì „í™˜ ì§€ì : {len(topic_changes)}ê°œ")
            return topic_changes
            
        except Exception as e:
            logger.error(f"ê°•í•œ ì£¼ì œ ì „í™˜ ê°ì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _detect_topic_changes(self, segments):
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì£¼ì œ ë³€í™” ê°ì§€"""
        try:
            topic_changes = []
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼ì œ ë³€í™” ê°ì§€
            for i in range(1, len(segments)):
                curr_text = segments[i].get("text", "").strip()
                prev_text = segments[i-1].get("text", "").strip()
                
                # ì£¼ì œ ë³€í™” ì‹ í˜¸ í‚¤ì›Œë“œ (í•œêµ­ì–´, ì¼ë³¸ì–´, ì˜ì–´)
                topic_change_keywords = [
                    # í•œêµ­ì–´
                    'ê·¸ëŸ°ë°', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•œí¸', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê²°êµ­', 'ë§ˆì§€ë§‰ìœ¼ë¡œ',
                    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë‹¤ìŒìœ¼ë¡œ', 'ì´ì œ', 'ê·¸ëŸ¼', 'ê·¸ëŸ¬ë©´',
                    # ì¼ë³¸ì–´  
                    'ãã‚Œã§', 'ãã—ã¦', 'ã¾ãŸ', 'ã—ã‹ã—', 'ã§ã‚‚', 'ã¨ã“ã‚ã§', 'ã•ã¦',
                    'ã¾ãš', 'æ¬¡ã«', 'æœ€å¾Œã«', 'çµå±€', 'ã¤ã¾ã‚Š', 'ã ã‹ã‚‰',
                    # ì˜ì–´
                    'however', 'but', 'and', 'also', 'then', 'next', 'finally',
                    'first', 'second', 'third', 'so', 'therefore', 'meanwhile'
                ]
                
                # í˜„ì¬ í…ìŠ¤íŠ¸ì— ì£¼ì œ ë³€í™” í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                curr_lower = curr_text.lower()
                if any(keyword in curr_lower for keyword in topic_change_keywords):
                    topic_changes.append(i)
                    logger.debug(f"ì£¼ì œ ë³€í™” ê°ì§€: ì„¸ê·¸ë¨¼íŠ¸ {i} - {curr_text[:30]}...")
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸‰ë³€ (ê¸´ ì„¤ëª… í›„ ì§§ì€ ìš”ì•½ ë“±)
                if len(prev_text) > 50 and len(curr_text) < 20:
                    topic_changes.append(i)
                    logger.debug(f"í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸‰ë³€ ê°ì§€: ì„¸ê·¸ë¨¼íŠ¸ {i}")
            
            logger.info(f"ì£¼ì œ ë³€í™” ì§€ì : {len(topic_changes)}ê°œ - {topic_changes}")
            return topic_changes
            
        except Exception as e:
            logger.error(f"ì£¼ì œ ë³€í™” ê°ì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_voice_features(self, audio_segment, sample_rate):
        """ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ (í”¼ì¹˜, ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬, MFCC)"""
        try:
            import librosa
            import numpy as np
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            if isinstance(audio_segment, torch.Tensor):
                audio_np = audio_segment.numpy()
            else:
                audio_np = audio_segment
            
            # 1. ê¸°ë³¸ ì£¼íŒŒìˆ˜ (F0) - í”¼ì¹˜
            f0 = librosa.yin(audio_np, fmin=50, fmax=400, sr=sample_rate)
            f0_mean = np.nanmean(f0[f0 > 0]) if np.any(f0 > 0) else 150
            f0_std = np.nanstd(f0[f0 > 0]) if np.any(f0 > 0) else 0
            
            # 2. MFCC (Mel-frequency cepstral coefficients) - ìŒìƒ‰ íŠ¹ì„±
            mfcc = librosa.feature.mfcc(y=audio_np, sr=sample_rate, n_mfcc=13)
            mfcc_mean = np.mean(mfcc, axis=1)
            mfcc_std = np.std(mfcc, axis=1)
            
            # 3. ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ (Spectral Centroid) - ìŒì„±ì˜ ë°ê¸°
            spectral_centroid = librosa.feature.spectral_centroid(y=audio_np, sr=sample_rate)
            sc_mean = np.mean(spectral_centroid)
            sc_std = np.std(spectral_centroid)
            
            # 4. ìŠ¤í™íŠ¸ëŸ´ ëŒ€ì—­í­ (Spectral Bandwidth)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_np, sr=sample_rate)
            sb_mean = np.mean(spectral_bandwidth)
            
            # 5. ì˜êµì°¨ìœ¨ (Zero Crossing Rate) - ìŒì„±ì˜ ê±°ì¹ ê¸°
            zcr = librosa.feature.zero_crossing_rate(audio_np)
            zcr_mean = np.mean(zcr)
            
            # íŠ¹ì„± ë²¡í„° êµ¬ì„±
            features = np.concatenate([
                [f0_mean, f0_std],           # í”¼ì¹˜ íŠ¹ì„± (2ì°¨ì›)
                mfcc_mean[:8],               # MFCC í‰ê·  (8ì°¨ì›)
                [sc_mean, sc_std],           # ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ (2ì°¨ì›)
                [sb_mean],                   # ìŠ¤í™íŠ¸ëŸ´ ëŒ€ì—­í­ (1ì°¨ì›)
                [zcr_mean]                   # ì˜êµì°¨ìœ¨ (1ì°¨ì›)
            ])
            
            # NaN ê°’ ì²˜ë¦¬
            features = np.nan_to_num(features, nan=0.0)
            
            logger.debug(f"ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: F0={f0_mean:.1f}Hz, SC={sc_mean:.1f}Hz")
            return features
            
        except ImportError:
            logger.warning("librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°„ë‹¨í•œ íŠ¹ì„±ë§Œ ì¶”ì¶œ")
            return self._extract_simple_voice_features(audio_segment, sample_rate)
        except Exception as e:
            logger.error(f"ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_simple_voice_features(self, audio_segment, sample_rate):
        """librosa ì—†ì´ ê°„ë‹¨í•œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ"""
        try:
            import numpy as np
            
            if isinstance(audio_segment, torch.Tensor):
                audio_np = audio_segment.numpy()
            else:
                audio_np = audio_segment
            
            # 1. RMS ì—ë„ˆì§€ (ìŒëŸ‰)
            rms_energy = np.sqrt(np.mean(audio_np**2))
            
            # 2. ì˜êµì°¨ìœ¨ (ìŒì„±ì˜ ê±°ì¹ ê¸°)
            zero_crossings = np.sum(np.diff(np.sign(audio_np)) != 0)
            zcr = zero_crossings / len(audio_np)
            
            # 3. ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ (FFT)
            fft = np.fft.fft(audio_np)
            magnitude = np.abs(fft[:len(fft)//2])
            freqs = np.fft.fftfreq(len(audio_np), 1/sample_rate)[:len(fft)//2]
            
            # ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ (ê°€ì¤‘ í‰ê·  ì£¼íŒŒìˆ˜)
            if np.sum(magnitude) > 0:
                spectral_centroid = np.sum(freqs * magnitude) / np.sum(magnitude)
            else:
                spectral_centroid = 0
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ (ìµœëŒ€ ì—ë„ˆì§€ ì£¼íŒŒìˆ˜)
            dominant_freq = freqs[np.argmax(magnitude)] if len(magnitude) > 0 else 0
            
            # íŠ¹ì„± ë²¡í„° êµ¬ì„±
            features = np.array([
                rms_energy,
                zcr,
                spectral_centroid,
                dominant_freq,
                np.mean(magnitude),
                np.std(magnitude)
            ])
            
            # NaN ê°’ ì²˜ë¦¬
            features = np.nan_to_num(features, nan=0.0)
            
            logger.debug(f"ê°„ë‹¨í•œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ: ì£¼íŒŒìˆ˜={dominant_freq:.1f}Hz, ì—ë„ˆì§€={rms_energy:.3f}")
            return features
            
        except Exception as e:
            logger.error(f"ê°„ë‹¨í•œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _cluster_voice_features(self, voice_features):
        """ìŒì„± íŠ¹ì„± ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§"""
        try:
            import numpy as np
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler
            
            features_array = np.array(voice_features)
            
            # íŠ¹ì„± ì •ê·œí™”
            scaler = StandardScaler()
            features_normalized = scaler.fit_transform(features_array)
            
            # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • - ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
            n_segments = len(voice_features)
            if n_segments <= 2:
                n_clusters = 1  # ë‹¨ì¼ í™”ì
            elif n_segments <= 4:
                n_clusters = 2  # 2ëª… í™”ì
            elif n_segments <= 7:
                n_clusters = min(2, n_segments - 1)  # ìµœëŒ€ 2ëª…
            else:
                n_clusters = min(3, n_segments // 2)  # ìµœëŒ€ 3ëª…
            
            if n_clusters == 1:
                logger.info("ë‹¨ì¼ í™”ìë¡œ í´ëŸ¬ìŠ¤í„°ë§")
                return [0] * len(voice_features)
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§ (ì—¬ëŸ¬ ë²ˆ ì‹œë„í•´ì„œ ìµœì  ê²°ê³¼ ì„ íƒ)
            best_labels = None
            best_inertia = float('inf')
            
            for attempt in range(5):  # 5ë²ˆ ì‹œë„
                try:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42+attempt, n_init=10)
                    labels = kmeans.fit_predict(features_normalized)
                    
                    if kmeans.inertia_ < best_inertia:
                        best_inertia = kmeans.inertia_
                        best_labels = labels
                        best_centers = kmeans.cluster_centers_
                except:
                    continue
            
            if best_labels is None:
                logger.warning("í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨ - ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬")
                return [0] * len(voice_features)
            
            logger.info(f"ìŒì„± íŠ¹ì„± í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {n_clusters}ëª… í™”ì ê°ì§€ (ê´€ì„±: {best_inertia:.2f})")
            
            # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê²€ì¦
            unique_labels = len(set(best_labels))
            if unique_labels < n_clusters:
                logger.warning(f"ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ê°€ ë¹„ì–´ìˆìŒ: {unique_labels}/{n_clusters}")
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ì •ë³´ ë¡œê¹…
            for i, center in enumerate(best_centers):
                logger.debug(f"í™”ì{chr(65+i)} íŠ¹ì„±: F0={center[0]:.1f}, MFCC1={center[2]:.2f}")
            
            return best_labels
            
        except ImportError:
            logger.warning("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°„ë‹¨í•œ ë¶„ë¥˜ ì‚¬ìš©")
            return self._simple_voice_clustering(voice_features)
        except Exception as e:
            logger.error(f"ìŒì„± íŠ¹ì„± í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return [0] * len(voice_features)  # ëª¨ë‘ ê°™ì€ í™”ìë¡œ ì²˜ë¦¬
    
    def _simple_voice_clustering(self, voice_features):
        """ê°„ë‹¨í•œ ìŒì„± íŠ¹ì„± ê¸°ë°˜ ë¶„ë¥˜"""
        try:
            import numpy as np
            
            features_array = np.array(voice_features)
            
            # ì²« ë²ˆì§¸ íŠ¹ì„± (í”¼ì¹˜ ë˜ëŠ” ì£¼íŒŒìˆ˜)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
            first_feature = features_array[:, 0]
            
            # ì¤‘ì•™ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ 2ê·¸ë£¹ ë¶„í• 
            median_value = np.median(first_feature)
            labels = (first_feature > median_value).astype(int)
            
            logger.info(f"ê°„ë‹¨í•œ ìŒì„± ë¶„ë¥˜ ì™„ë£Œ: ê¸°ì¤€ê°’={median_value:.2f}")
            return labels
            
        except Exception as e:
            logger.error(f"ê°„ë‹¨í•œ ìŒì„± ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return [0] * len(voice_features)
    
    def _assign_speakers_from_clusters(self, segments, valid_segments, speaker_labels):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì— í• ë‹¹"""
        speaker_assignments = []
        label_to_speaker = {}
        
        # ë¼ë²¨ì„ í™”ì ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
        unique_labels = sorted(set(speaker_labels))
        for i, label in enumerate(unique_labels):
            speaker_letter = chr(ord('A') + i)
            label_to_speaker[label] = f"í™”ì{speaker_letter}"
        
        # ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì¸ë±ìŠ¤ì™€ ë¼ë²¨ ë§¤í•‘
        valid_assignments = {}
        for (seg_idx, segment), label in zip(valid_segments, speaker_labels):
            valid_assignments[seg_idx] = label_to_speaker[label]
        
        # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì í• ë‹¹
        current_speaker = "í™”ìA"
        for i, segment in enumerate(segments):
            if i in valid_assignments:
                current_speaker = valid_assignments[i]
            
            speaker_assignments.append(current_speaker)
        
        # í™”ì ì¼ê´€ì„± í›„ì²˜ë¦¬
        speaker_assignments = self._post_process_voice_consistency(speaker_assignments, valid_segments, speaker_labels)
        
        # ê²°ê³¼ ë¡œê¹…
        from collections import Counter
        speaker_count = Counter(speaker_assignments)
        logger.info(f"ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ì ë¶„í¬: {dict(speaker_count)}")
        
        return speaker_assignments
    
    def _post_process_voice_consistency(self, speaker_assignments, valid_segments, speaker_labels):
        """ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ì ì¼ê´€ì„± í›„ì²˜ë¦¬"""
        try:
            from collections import Counter
            import numpy as np
            
            logger.info("ìŒì„± íŠ¹ì„± ê¸°ë°˜ ì¼ê´€ì„± í›„ì²˜ë¦¬ ì‹œì‘...")
            
            # í™”ìë³„ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ í™•ì¸
            speaker_counts = Counter(speaker_assignments)
            logger.info(f"í›„ì²˜ë¦¬ ì „ í™”ì ë¶„í¬: {dict(speaker_counts)}")
            
            # í™”ìë¶„ë¦¬ í’ˆì§ˆ í™•ì¸ - ë„ˆë¬´ ì ê·¹ì ì¸ í†µí•© ë°©ì§€
            total_speakers = len(speaker_counts)
            if total_speakers <= 2:
                logger.info(f"í™”ì ìˆ˜ê°€ ì ì ˆí•¨ ({total_speakers}ëª…) - í›„ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°")
                return speaker_assignments
            
            # ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ê°€ì§„ í™”ìë“¤ ì°¾ê¸° (ë” ì‹ ì¤‘í•˜ê²Œ)
            isolated_speakers = [speaker for speaker, count in speaker_counts.items() if count == 1]
            
            if not isolated_speakers:
                logger.info("ê³ ë¦½ëœ í™”ì ì—†ìŒ - í›„ì²˜ë¦¬ ì™„ë£Œ")
                return speaker_assignments
            
            logger.info(f"ê³ ë¦½ëœ í™”ì ë°œê²¬: {isolated_speakers}")
            
            # ê³ ë¦½ëœ í™”ì í†µí•© - ë” ì‹ ì¤‘í•œ ì¡°ê±´
            assignments = speaker_assignments.copy()
            
            for isolated_speaker in isolated_speakers:
                isolated_index = speaker_assignments.index(isolated_speaker)
                
                # ì•ë’¤ í™”ì í™•ì¸
                prev_speaker = None
                next_speaker = None
                
                if isolated_index > 0:
                    prev_speaker = assignments[isolated_index - 1]
                if isolated_index < len(assignments) - 1:
                    next_speaker = assignments[isolated_index + 1]
                
                # í†µí•© ëŒ€ìƒ ê²°ì • - ë” ì—„ê²©í•œ ì¡°ê±´
                target_speaker = None
                
                # 1. ì•ë’¤ê°€ ì •í™•íˆ ê°™ì€ í™”ìì´ê³ , ê·¸ í™”ìê°€ 3ê°œ ì´ìƒ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê°€ì§„ ê²½ìš°ë§Œ í†µí•©
                if (prev_speaker and prev_speaker == next_speaker and 
                    speaker_counts[prev_speaker] >= 3):
                    target_speaker = prev_speaker
                    logger.info(f"ê³ ë¦½ëœ í™”ì {isolated_speaker} â†’ {target_speaker} (ì•ë’¤ ë™ì¼, ì£¼ìš”í™”ì)")
                
                # 2. ë‹¤ë¥¸ ê²½ìš°ëŠ” í†µí•©í•˜ì§€ ì•ŠìŒ (í™”ì ë‹¤ì–‘ì„± ë³´ì¡´)
                else:
                    logger.info(f"ê³ ë¦½ëœ í™”ì {isolated_speaker} ìœ ì§€ (í™”ì ë‹¤ì–‘ì„± ë³´ì¡´)")
                
                # í†µí•© ì‹¤í–‰
                if target_speaker:
                    assignments[isolated_index] = target_speaker
            
            # ìµœì¢… ê²°ê³¼ ë¡œê¹…
            final_counts = Counter(assignments)
            logger.info(f"í›„ì²˜ë¦¬ í›„ í™”ì ë¶„í¬: {dict(final_counts)}")
            
            return assignments
            
        except Exception as e:
            logger.error(f"ìŒì„± íŠ¹ì„± ì¼ê´€ì„± í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return speaker_assignments
    
    def _fallback_speaker_assignment(self, segments):
        """ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ë¡œì§"""
        logger.info("ëŒ€ì•ˆ í™”ì í• ë‹¹ ë¡œì§ ì‚¬ìš©")
        
        # 1.5ì´ˆ ì´ìƒ ì¹¨ë¬µ ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¨ ë¶„ë¦¬
        speaker_assignments = []
        current_speaker = 'A'
        
        for i, segment in enumerate(segments):
            if i > 0:
                prev_end = segments[i-1].get("end", 0)
                curr_start = segment.get("start", 0)
                silence_duration = curr_start - prev_end
                
                if silence_duration > 1.5:
                    current_speaker = 'B' if current_speaker == 'A' else 'A'
            
            speaker_assignments.append(f"í™”ì{current_speaker}")
        
        return speaker_assignments
    
    def _is_single_speaker(self, segments):
        """ê°„ë‹¨í•œ ë‹¨ì¼ í™”ì íŒë‹¨ ë¡œì§"""
        if len(segments) <= 2:
            logger.info("ì„¸ê·¸ë¨¼íŠ¸ 2ê°œ ì´í•˜ - ë‹¨ì¼ í™”ìë¡œ íŒë‹¨")
            return True
        
        # 1.5ì´ˆ ì´ìƒ ì¹¨ë¬µì´ ìˆëŠ”ì§€ë§Œ í™•ì¸
        long_silence_count = 0
        
        for i in range(1, len(segments)):
            prev_end = segments[i-1].get("end", 0)
            curr_start = segments[i].get("start", 0)
            silence_duration = curr_start - prev_end
            
            if silence_duration > 1.5:
                long_silence_count += 1
        
        # ê¸´ ì¹¨ë¬µì´ ì—†ìœ¼ë©´ ë‹¨ì¼ í™”ì
        is_single = long_silence_count == 0
        
        logger.info(f"ë‹¨ì¼ í™”ì íŒë‹¨: {'ë‹¨ì¼' if is_single else 'ë‹¤ì¤‘'} "
                   f"(1.5ì´ˆ+ ì¹¨ë¬µ: {long_silence_count}íšŒ, ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ)")
        
        return is_single
    
    def _extract_speaker_embeddings(self, audio_file, segments):
        """ECAPA-VOXCELEBë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ê·¸ë¨¼íŠ¸ë³„ í™”ì ì„ë² ë”© ì¶”ì¶œ"""
        if not self._load_speaker_encoder():
            logger.warning("í™”ìë¶„ë¦¬ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê·œì¹™ ê¸°ë°˜ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
            return None
        
        try:
            # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
            waveform, sample_rate = torchaudio.load(audio_file)
            
            # ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ (ECAPA ëª¨ë¸ ìš”êµ¬ì‚¬í•­)
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
                sample_rate = 16000
            
            embeddings = []
            valid_segments = []
            
            for segment in segments:
                start_time = segment.get("start", 0)
                end_time = segment.get("end", start_time + 1)
                text = segment.get("text", "").strip()
                
                if not text or end_time <= start_time:
                    continue
                
                # ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ ì¶”ì¶œ
                start_sample = int(start_time * sample_rate)
                end_sample = int(end_time * sample_rate)
                
                if start_sample >= waveform.shape[1] or end_sample <= start_sample:
                    continue
                
                segment_audio = waveform[:, start_sample:end_sample]
                
                # ë„ˆë¬´ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê±´ë„ˆë›°ê¸° (ìµœì†Œ 0.5ì´ˆ)
                if segment_audio.shape[1] < sample_rate * 0.5:
                    continue
                
                # í™”ì ì„ë² ë”© ì¶”ì¶œ
                try:
                    embedding = self.speaker_encoder.encode_batch(segment_audio.to(self.device))
                    embeddings.append(embedding.squeeze().cpu().numpy())
                    valid_segments.append(segment)
                except Exception as e:
                    logger.warning(f"ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            if not embeddings:
                logger.warning("ìœ íš¨í•œ í™”ì ì„ë² ë”©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            return np.array(embeddings), valid_segments
            
        except Exception as e:
            logger.error(f"í™”ì ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _cluster_speakers(self, embeddings, min_speakers=2, max_speakers=5):
        """í™”ì ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ í™”ì êµ¬ë¶„"""
        try:
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(embeddings)
            
            # ê±°ë¦¬ í–‰ë ¬ë¡œ ë³€í™˜ (1 - ìœ ì‚¬ë„)
            distance_matrix = 1 - similarity_matrix
            
            # ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
            best_n_clusters = min_speakers
            best_score = -1
            
            for n_clusters in range(min_speakers, min(max_speakers + 1, len(embeddings) + 1)):
                try:
                    clustering = AgglomerativeClustering(
                        n_clusters=n_clusters,
                        metric='precomputed',
                        linkage='average'
                    )
                    labels = clustering.fit_predict(distance_matrix)
                    
                    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚° (ê°„ë‹¨í•œ í‰ê°€)
                    if len(set(labels)) > 1:
                        from sklearn.metrics import silhouette_score
                        score = silhouette_score(distance_matrix, labels, metric='precomputed')
                        if score > best_score:
                            best_score = score
                            best_n_clusters = n_clusters
                except:
                    continue
            
            # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
            clustering = AgglomerativeClustering(
                n_clusters=best_n_clusters,
                metric='precomputed',
                linkage='average'
            )
            labels = clustering.fit_predict(distance_matrix)
            
            logger.info(f"í™”ì í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {best_n_clusters}ëª…ì˜ í™”ì ê°ì§€")
            return labels
            
        except Exception as e:
            logger.error(f"í™”ì í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def _assign_ecapa_speakers(self, audio_file, segments):
        """ECAPA-VOXCELEB ê¸°ë°˜ ì‹¤ì œ í™”ìë¶„ë¦¬"""
        # í™”ì ì„ë² ë”© ì¶”ì¶œ
        embedding_result = self._extract_speaker_embeddings(audio_file, segments)
        
        if embedding_result is None:
            logger.warning("ECAPA í™”ìë¶„ë¦¬ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©")
            return self._assign_smart_speakers(segments)
        
        embeddings, valid_segments = embedding_result
        
        # í™”ì í´ëŸ¬ìŠ¤í„°ë§
        cluster_labels = self._cluster_speakers(embeddings)
        
        if cluster_labels is None:
            logger.warning("í™”ì í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©")
            return self._assign_smart_speakers(segments)
        
        # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ í™”ì ì´ë¦„ìœ¼ë¡œ ë³€í™˜
        unique_labels = sorted(set(cluster_labels))
        label_to_speaker = {}
        
        for i, label in enumerate(unique_labels):
            speaker_letter = chr(ord('A') + i)
            label_to_speaker[label] = f"í™”ì{speaker_letter}"
        
        # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì í• ë‹¹
        speaker_assignments = []
        valid_idx = 0
        
        for segment in segments:
            text = segment.get("text", "").strip()
            if not text:
                # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì´ì „ í™”ì ìœ ì§€
                if speaker_assignments:
                    speaker_assignments.append(speaker_assignments[-1])
                else:
                    speaker_assignments.append("í™”ìA")
                continue
            
            # ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ì¸ì§€ í™•ì¸
            if valid_idx < len(valid_segments) and segment == valid_segments[valid_idx]:
                # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‚¬ìš©
                cluster_label = cluster_labels[valid_idx]
                speaker_name = label_to_speaker[cluster_label]
                speaker_assignments.append(speaker_name)
                valid_idx += 1
            else:
                # ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì´ì „ í™”ì ìœ ì§€
                if speaker_assignments:
                    speaker_assignments.append(speaker_assignments[-1])
                else:
                    speaker_assignments.append("í™”ìA")
        
        return speaker_assignments
    
    def process_single_file(self, input_file):
        """
        ë‹¨ì¼ íŒŒì¼ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ (ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ ì§€ì›)
        
        Args:
            input_file (str): ì…ë ¥ ì˜¤ë””ì˜¤ ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        try:
            input_path = Path(input_file)
            file_stem = input_path.stem
            
            # íŒŒì¼ íƒ€ì… í™•ì¸ ë° ì „ì²˜ë¦¬
            if self._is_video_file(input_file):
                logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ íŒŒì¼ ê°ì§€: {input_file}")
                # ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
                audio_file = self.extract_audio_from_video(input_file)
                logger.info(f"ğŸ“„ ì²˜ë¦¬í•  ì˜¤ë””ì˜¤ íŒŒì¼: {audio_file}")
            elif self._is_audio_file(input_file):
                logger.info(f"ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì§€: {input_file}")
                audio_file = input_file
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {input_path.suffix}")
            
            # 1ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°
            denoised_file = self.audio_out_dir / f"{file_stem}_denoised.wav"
            self.denoise_audio(audio_file, str(denoised_file))
            
            # 2ë‹¨ê³„: audio_outputìœ¼ë¡œ ë³µì‚¬ (íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ìœ ì§€)
            output_audio_file = self.audio_output_dir / f"{file_stem}_denoised.wav"
            import shutil
            shutil.copy2(denoised_file, output_audio_file)
            
            # 3ë‹¨ê³„: STT ì²˜ë¦¬
            transcript_file = self.script_output_dir / f"{file_stem}_transcript.txt"
            srt_file = self.script_output_dir / f"{file_stem}_subtitle.srt"
            transcribed_text = self.transcribe_audio(output_audio_file, transcript_file, srt_file)
            
            logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {input_file}")
            return transcribed_text
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({input_file}): {e}")
            raise
    
    def process_all_files(self):
        """audio_input í´ë”ì˜ ëª¨ë“  ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì (ì˜¤ë””ì˜¤ + ë¹„ë””ì˜¤)
        file_extensions = ['*.wav', '*.mp3', '*.m4a', '*.flac', '*.ogg', '*.aac',
                          '*.mp4', '*.avi', '*.mov', '*.mkv', '*.webm', '*.flv', '*.wmv']
        
        media_files = []
        for ext in file_extensions:
            media_files.extend(glob.glob(str(self.audio_input_dir / ext)))
        
        if not media_files:
            logger.warning(f"audio_input í´ë”ì— ì²˜ë¦¬í•  ë¯¸ë””ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info(f"ì§€ì› í˜•ì‹: {', '.join(file_extensions)}")
            return
        
        # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
        audio_files = [f for f in media_files if self._is_audio_file(f)]
        video_files = [f for f in media_files if self._is_video_file(f)]
        
        logger.info(f"ì´ {len(media_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"  - ì˜¤ë””ì˜¤ íŒŒì¼: {len(audio_files)}ê°œ")
        logger.info(f"  - ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
        
        if video_files and not self.ffmpeg_available:
            logger.warning(f"âš ï¸ FFmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë¹„ë””ì˜¤ íŒŒì¼ {len(video_files)}ê°œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            media_files = audio_files
        
        results = []
        for i, media_file in enumerate(media_files, 1):
            try:
                file_type = "ğŸ¬ ë¹„ë””ì˜¤" if self._is_video_file(media_file) else "ğŸµ ì˜¤ë””ì˜¤"
                logger.info(f"[{i}/{len(media_files)}] {file_type} ì²˜ë¦¬ ì¤‘: {Path(media_file).name}")
                result = self.process_single_file(media_file)
                results.append({
                    'file': Path(media_file).name,
                    'type': 'video' if self._is_video_file(media_file) else 'audio',
                    'status': 'success',
                    'text': result
                })
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {Path(media_file).name} - {e}")
                results.append({
                    'file': Path(media_file).name,
                    'type': 'video' if self._is_video_file(media_file) else 'audio',
                    'status': 'failed',
                    'error': str(e)
                })
        
        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥
        self._save_processing_summary(results)
        
        logger.info("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
        return results
    
    def _save_processing_summary(self, results):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥"""
        summary_file = self.script_output_dir / f"processing_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=== ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ===\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ íŒŒì¼ ìˆ˜: {len(results)}\n")
            
            success_count = sum(1 for r in results if r['status'] == 'success')
            failed_count = len(results) - success_count
            
            f.write(f"ì„±ê³µ: {success_count}ê°œ\n")
            f.write(f"ì‹¤íŒ¨: {failed_count}ê°œ\n")
            f.write("-" * 50 + "\n\n")
            
            for result in results:
                f.write(f"íŒŒì¼: {result['file']}\n")
                f.write(f"ìƒíƒœ: {result['status']}\n")
                if result['status'] == 'success':
                    f.write(f"í…ìŠ¤íŠ¸: {result['text'][:200]}...\n")
                else:
                    f.write(f"ì˜¤ë¥˜: {result['error']}\n")
                f.write("-" * 30 + "\n")
        
        logger.info(f"ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥: {summary_file}")

def main(target_language=None):
    """
    ë©”ì¸ í•¨ìˆ˜
    
    Args:
        target_language (str): ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ ('ko', 'ja', 'en', etc.)
    """
    print("=== ìŒì„± ë° ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
    print("ğŸµ ì˜¤ë””ì˜¤: audio_input â†’ ë…¸ì´ì¦ˆì œê±° â†’ audio_out â†’ STT â†’ script_output")
    print("ğŸ¬ ë¹„ë””ì˜¤: video_input â†’ ì˜¤ë””ì˜¤ì¶”ì¶œ â†’ ë…¸ì´ì¦ˆì œê±° â†’ STT â†’ script_output")
    print()
    
    # ì–¸ì–´ ì„¤ì • ì•ˆë‚´
    if target_language:
        supported_languages = {
            'ko': 'í•œêµ­ì–´', 'ja': 'æ—¥æœ¬èª', 'en': 'English', 'zh': 'ä¸­æ–‡',
            'es': 'EspaÃ±ol', 'fr': 'FranÃ§ais', 'de': 'Deutsch', 'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹'
        }
        lang_name = supported_languages.get(target_language, target_language)
        print(f"ğŸŒ ëŒ€ìƒ ì–¸ì–´: {lang_name} ({target_language})")
    else:
        print("ğŸŒ ì–¸ì–´: ìë™ ê°ì§€ ëª¨ë“œ")
    print()
    
    try:
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        use_gpu = torch.cuda.is_available()
        if use_gpu:
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = AudioPipeline(use_gpu=use_gpu, target_language=target_language)
        
        # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
        results = pipeline.process_all_files()
        
        # ê²°ê³¼ ì¶œë ¥
        if results:
            success_count = sum(1 for r in results if r['status'] == 'success')
            print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(results)}ê°œ íŒŒì¼ ì„±ê³µ")
        else:
            print("\nâš ï¸  ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("audio_input í´ë”ì— ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

# ì‚¬ìš© ì˜ˆì‹œ:
# 
# 1. ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬:
#    audio_input/ í´ë”ì— WAV, MP3, M4A, FLAC, OGG íŒŒì¼ ì €ì¥ í›„ ì‹¤í–‰
#
# 2. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (FFmpeg í•„ìš”):
#    audio_input/ í´ë”ì— MP4, AVI, MOV, MKV, WEBM íŒŒì¼ ì €ì¥ í›„ ì‹¤í–‰
#    
# 3. íŠ¹ì • ì–¸ì–´ ì§€ì •:
#    pipeline = AudioPipeline(target_language='ko')  # í•œêµ­ì–´
#    pipeline = AudioPipeline(target_language='ja')  # ì¼ë³¸ì–´
#
# 4. ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬:
#    pipeline = AudioPipeline()
#    result = pipeline.process_single_file("video.mp4")
#
# 5. FFmpeg ì„¤ì¹˜ í™•ì¸:
#    pipeline = AudioPipeline()
#    print(f"FFmpeg ì‚¬ìš© ê°€ëŠ¥: {pipeline.ffmpeg_available}")
#
# ì¶œë ¥ íŒŒì¼:
# - audio_out/: ë…¸ì´ì¦ˆ ì œê±°ëœ ì˜¤ë””ì˜¤
# - script_output/: ì „ì‚¬ í…ìŠ¤íŠ¸ ë° SRT ìë§‰
# - processing_summary_*.txt: ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
